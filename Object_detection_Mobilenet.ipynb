{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094563ce",
   "metadata": {},
   "source": [
    "# Image AR Project for MyData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69fa6b7",
   "metadata": {},
   "source": [
    "- Start date : 2022.01.18\n",
    "- Author : Hyoungjoon Lim\n",
    "- Image crawling for scene text classification (200 images per class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c172a78",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c729e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import urllib.request\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import splitfolders\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac911a0f",
   "metadata": {},
   "source": [
    "## 2. Input keywords (list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63342d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['우리은행 뉴스']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdc7d6d",
   "metadata": {},
   "source": [
    "## 3. Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc36fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDirectory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "\n",
    "def crawling_img(name):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.google.co.kr/imghp?hl=ko&tab=wi&authuser=0&ogbl\")\n",
    "    elem = driver.find_element_by_name(\"q\")\n",
    "    elem.send_keys(name)\n",
    "    elem.send_keys(Keys.RETURN)\n",
    "\n",
    "    #\n",
    "    SCROLL_PAUSE_TIME = 1\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")  # 브라우저의 높이를 자바스크립트로 찾음\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")  # 브라우저 끝까지 스크롤을 내림\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            try:\n",
    "                driver.find_element_by_css_selector(\".mye4qd\").click()\n",
    "            except:\n",
    "                break\n",
    "        last_height = new_height\n",
    "\n",
    "    imgs = driver.find_elements_by_css_selector(\".rg_i.Q4LuWd\")\n",
    "    dir = \".\\crawled_image\" + \"\\\\\" + name\n",
    "\n",
    "    createDirectory(dir) #폴더 생성\n",
    "    count = 1\n",
    "    for img in imgs:\n",
    "        try:\n",
    "            img.click()\n",
    "            time.sleep(2)\n",
    "            imgUrl = driver.find_element_by_xpath(\n",
    "                '//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[1]/div[2]/div[1]/a/img').get_attribute(\n",
    "                \"src\")\n",
    "            path = \"C:\\\\Users\\\\Hyoungjoon LIM\\\\Desktop\\\\scene_text_classification\\\\crawled_image\\\\\" + name + \"\\\\\"\n",
    "            urllib.request.urlretrieve(imgUrl, path + name + str(count) + \".jpg\")\n",
    "            count = count + 1\n",
    "            if count > 200:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b8964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in keywords:\n",
    "    crawling_img(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabefc07",
   "metadata": {},
   "source": [
    "## 4. Rename .jpg files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "433daeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\tf1\\\\Lib\\\\site-packages\\\\tensorflow\\\\models\\\\image\\\\'\n",
    "counter = 1\n",
    "for f in os.listdir(path):\n",
    "    print(f)\n",
    "    suffix = f.split('.')[-1]\n",
    "    if suffix == 'jpg' or suffix == 'png':\n",
    "        new = '{}.{}'.format(str(counter), suffix)\n",
    "        os.rename(path + f, path + new)\n",
    "        counter = int(counter) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e7cbb",
   "metadata": {},
   "source": [
    "#### 4.1. If you wanna add sth..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a26d397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KakaoTalk_20220214_145842518.jpg\n",
      "KakaoTalk_20220214_145842518_01.jpg\n",
      "KakaoTalk_20220214_145842518_02.jpg\n",
      "KakaoTalk_20220214_145842518_03.jpg\n",
      "KakaoTalk_20220214_145842518_04.jpg\n",
      "KakaoTalk_20220214_145842518_05.jpg\n",
      "KakaoTalk_20220214_145842518_06.jpg\n",
      "KakaoTalk_20220214_145842518_07.jpg\n",
      "KakaoTalk_20220214_145842518_08.jpg\n",
      "KakaoTalk_20220214_145842518_09.jpg\n",
      "KakaoTalk_20220214_145842518_10.jpg\n",
      "KakaoTalk_20220214_145842518_11.jpg\n",
      "KakaoTalk_20220214_145842518_12.jpg\n",
      "KakaoTalk_20220214_145842518_13.jpg\n",
      "KakaoTalk_20220214_145842518_14.jpg\n",
      "KakaoTalk_20220214_145842518_15.jpg\n",
      "KakaoTalk_20220214_145842518_16.jpg\n",
      "KakaoTalk_20220214_145842518_17.jpg\n",
      "KakaoTalk_20220214_145842518_18.jpg\n",
      "KakaoTalk_20220214_145842518_19.jpg\n",
      "KakaoTalk_20220214_145842518_20.jpg\n",
      "KakaoTalk_20220214_145842518_21.jpg\n",
      "KakaoTalk_20220214_145842518_22.jpg\n",
      "KakaoTalk_20220214_145842518_23.jpg\n",
      "KakaoTalk_20220214_145842518_24.jpg\n",
      "KakaoTalk_20220214_145842518_25.jpg\n",
      "KakaoTalk_20220214_145842518_26.jpg\n",
      "KakaoTalk_20220214_145842518_27.jpg\n",
      "KakaoTalk_20220214_145842518_28.jpg\n",
      "KakaoTalk_20220214_145842518_29.jpg\n",
      "KakaoTalk_20220214_145941278.jpg\n",
      "KakaoTalk_20220214_145941278_01.jpg\n",
      "KakaoTalk_20220214_145941278_02.jpg\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\Hyoungjoon LIM\\Desktop\\scene_text_classification\\직원추가\\\\'\n",
    "counter = 4142 # start_number\n",
    "for f in os.listdir(path):\n",
    "    print(f)\n",
    "    suffix = f.split('.')[-1]\n",
    "    if suffix == 'jpg' or suffix == 'png' or suffix == 'jpeg':\n",
    "        new = '{}.{}'.format(str(counter), suffix)\n",
    "        os.rename(path + f, path + new)\n",
    "        counter = int(counter) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e84d5",
   "metadata": {},
   "source": [
    "## 5. Data preparation\n",
    "\n",
    "### 5.1 Image split (70%, 30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a373e2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 1625 files [01:02, 25.83 files/s]\n"
     ]
    }
   ],
   "source": [
    "splitfolders.ratio(r\"C:\\Users\\Hyoungjoon LIM\\Desktop\\scene_text_classification\\yolo_label\\all_data\", \n",
    "                    output=r\"C:\\Users\\Hyoungjoon LIM\\Desktop\\scene_text_classification\\yolo_label\\all_data\",\n",
    "                    seed=19, ratio=(.7,.3), group_prefix=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8833cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindir = r\"C:\\Users\\Hyoungjoon LIM\\Desktop\\scene_text_classification\\yolo_label\\all_data\\train\\class\"\n",
    "testdir = r\"C:\\Users\\Hyoungjoon LIM\\Desktop\\scene_text_classification\\yolo_label\\all_data\\val\\class\"\n",
    "\n",
    "train_list,test_list = os.listdir(traindir), os.listdir(testdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f0a14ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137 488\n"
     ]
    }
   ],
   "source": [
    "print(int(len(train_list)/2), int(len(test_list)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fee533",
   "metadata": {},
   "source": [
    "### 5.2 Modifying xml directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aabb6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetdir1 = r\"C:\\Users\\Hyoungjoon LIM\\Desktop\\scene_text_classification\\xml\" # XML dir\n",
    "\n",
    "file_list = os.listdir(targetdir1)\n",
    "xml_list = []\n",
    "for file in file_list:\n",
    "    if '.xml' in file:\n",
    "        xml_list.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f84f52c",
   "metadata": {},
   "source": [
    "#### 5.2.1. xml dir initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b874d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xml_file in xml_list:\n",
    "    target_path = targetdir1 + \"\\\\\" + xml_file\n",
    "    targetXML = open(target_path, 'rt', encoding='UTF8')\n",
    "\n",
    "    tree = ET.parse(targetXML)\n",
    "\n",
    "    root = tree.getroot()\n",
    "\n",
    "    ##수정할 부분\n",
    "\n",
    "    folder_tag = root.find(\"folder\")\n",
    "    path_tag = root.find(\"path\")\n",
    "    name_tag = root.find(\"filename\")\n",
    "    name = name_tag.text\n",
    "    \n",
    "    path_tag.text = r\"C:\\ProgramData\\Anaconda3\\envs\\tf1\\Lib\\site-packages\\tensorflow\\models\\image\\class\"+\"\\\\\"+name  #수정\n",
    "      \n",
    "    tree.write(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb3070",
   "metadata": {},
   "source": [
    "#### 5.2.2. modifying train/test directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42b8216a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "2 0\n",
      "3 0\n",
      "4 0\n",
      "5 0\n",
      "6 0\n",
      "7 0\n",
      "7 1\n",
      "8 1\n",
      "9 1\n",
      "9 2\n",
      "10 2\n",
      "11 2\n",
      "12 2\n",
      "13 2\n",
      "14 2\n",
      "15 2\n",
      "15 3\n",
      "15 4\n",
      "16 4\n",
      "17 4\n",
      "18 4\n",
      "18 5\n",
      "18 6\n",
      "19 6\n",
      "19 7\n",
      "19 8\n",
      "20 8\n",
      "21 8\n",
      "22 8\n",
      "23 8\n",
      "24 8\n",
      "24 9\n",
      "25 9\n",
      "26 9\n",
      "27 9\n",
      "27 10\n",
      "28 10\n",
      "29 10\n",
      "30 10\n",
      "30 11\n",
      "31 11\n",
      "32 11\n",
      "32 12\n",
      "33 12\n",
      "34 12\n",
      "35 12\n",
      "36 12\n",
      "37 12\n",
      "38 12\n",
      "39 12\n",
      "40 12\n",
      "41 12\n",
      "42 12\n",
      "43 12\n",
      "44 12\n",
      "45 12\n",
      "45 13\n",
      "46 13\n",
      "47 13\n",
      "48 13\n",
      "49 13\n",
      "50 13\n",
      "51 13\n",
      "51 14\n",
      "52 14\n",
      "53 14\n",
      "54 14\n",
      "54 15\n",
      "55 15\n",
      "55 16\n",
      "56 16\n",
      "56 17\n",
      "57 17\n",
      "58 17\n",
      "59 17\n",
      "59 18\n",
      "60 18\n",
      "61 18\n",
      "61 19\n",
      "62 19\n",
      "63 19\n",
      "64 19\n",
      "65 19\n",
      "66 19\n",
      "67 19\n",
      "68 19\n",
      "69 19\n",
      "69 20\n",
      "70 20\n",
      "71 20\n",
      "71 21\n",
      "71 22\n",
      "72 22\n",
      "72 23\n",
      "72 24\n",
      "72 25\n",
      "73 25\n",
      "74 25\n",
      "75 25\n",
      "76 25\n",
      "77 25\n",
      "77 26\n",
      "78 26\n",
      "79 26\n",
      "80 26\n",
      "81 26\n",
      "82 26\n",
      "83 26\n",
      "83 27\n",
      "84 27\n",
      "85 27\n",
      "86 27\n",
      "86 28\n",
      "87 28\n",
      "88 28\n",
      "88 29\n",
      "88 30\n",
      "89 30\n",
      "90 30\n",
      "90 31\n",
      "91 31\n",
      "91 32\n",
      "91 33\n",
      "92 33\n",
      "92 34\n",
      "93 34\n",
      "93 35\n",
      "93 36\n",
      "94 36\n",
      "95 36\n",
      "96 36\n",
      "97 36\n",
      "98 36\n",
      "99 36\n",
      "100 36\n",
      "101 36\n",
      "102 36\n",
      "103 36\n",
      "104 36\n",
      "105 36\n",
      "106 36\n",
      "107 36\n",
      "108 36\n",
      "109 36\n",
      "110 36\n",
      "111 36\n",
      "111 37\n",
      "112 37\n",
      "113 37\n",
      "114 37\n",
      "115 37\n",
      "115 38\n",
      "116 38\n",
      "116 39\n",
      "117 39\n",
      "118 39\n",
      "119 39\n",
      "120 39\n",
      "121 39\n",
      "121 40\n",
      "121 41\n",
      "122 41\n",
      "123 41\n",
      "124 41\n",
      "124 42\n",
      "125 42\n",
      "125 43\n",
      "125 44\n",
      "126 44\n",
      "127 44\n",
      "128 44\n",
      "129 44\n",
      "129 45\n",
      "130 45\n",
      "131 45\n",
      "132 45\n",
      "133 45\n",
      "134 45\n",
      "135 45\n",
      "136 45\n",
      "136 46\n",
      "136 47\n",
      "137 47\n",
      "138 47\n",
      "138 48\n",
      "139 48\n",
      "139 49\n",
      "140 49\n",
      "140 50\n",
      "141 50\n",
      "142 50\n",
      "142 51\n",
      "142 52\n",
      "143 52\n",
      "144 52\n",
      "145 52\n",
      "146 52\n",
      "147 52\n",
      "147 53\n",
      "148 53\n",
      "148 54\n",
      "149 54\n",
      "150 54\n",
      "150 55\n",
      "150 56\n",
      "151 56\n",
      "152 56\n",
      "153 56\n",
      "153 57\n",
      "153 58\n",
      "154 58\n",
      "154 59\n",
      "155 59\n",
      "156 59\n",
      "157 59\n",
      "158 59\n",
      "159 59\n",
      "160 59\n",
      "161 59\n",
      "161 60\n",
      "161 61\n",
      "162 61\n",
      "163 61\n",
      "164 61\n",
      "165 61\n",
      "166 61\n",
      "166 62\n",
      "167 62\n",
      "167 63\n",
      "168 63\n",
      "168 64\n",
      "169 64\n",
      "170 64\n",
      "170 65\n",
      "171 65\n",
      "172 65\n",
      "173 65\n",
      "174 65\n",
      "175 65\n",
      "176 65\n",
      "177 65\n",
      "178 65\n",
      "178 66\n",
      "178 67\n",
      "179 67\n",
      "180 67\n",
      "181 67\n",
      "182 67\n",
      "183 67\n",
      "184 67\n",
      "185 67\n",
      "186 67\n",
      "186 68\n",
      "187 68\n",
      "188 68\n",
      "188 69\n",
      "189 69\n",
      "190 69\n",
      "190 70\n",
      "191 70\n",
      "192 70\n",
      "193 70\n",
      "194 70\n",
      "195 70\n",
      "196 70\n",
      "197 70\n",
      "197 71\n",
      "198 71\n",
      "198 72\n",
      "199 72\n",
      "199 73\n",
      "200 73\n",
      "201 73\n",
      "202 73\n",
      "202 74\n",
      "203 74\n",
      "203 75\n",
      "204 75\n",
      "204 76\n",
      "204 77\n",
      "205 77\n",
      "205 78\n",
      "206 78\n",
      "207 78\n",
      "208 78\n",
      "208 79\n",
      "209 79\n",
      "210 79\n",
      "210 80\n",
      "210 81\n",
      "210 82\n",
      "211 82\n",
      "211 83\n",
      "212 83\n",
      "213 83\n",
      "214 83\n",
      "215 83\n",
      "216 83\n",
      "216 84\n",
      "216 85\n",
      "217 85\n",
      "218 85\n",
      "219 85\n",
      "220 85\n",
      "221 85\n",
      "222 85\n",
      "223 85\n",
      "224 85\n",
      "224 86\n",
      "225 86\n",
      "225 87\n",
      "226 87\n",
      "227 87\n",
      "227 88\n",
      "228 88\n",
      "229 88\n",
      "230 88\n",
      "231 88\n",
      "232 88\n",
      "233 88\n",
      "233 89\n",
      "233 90\n",
      "234 90\n",
      "235 90\n",
      "236 90\n",
      "237 90\n",
      "238 90\n",
      "238 91\n",
      "239 91\n",
      "239 92\n",
      "240 92\n",
      "240 93\n",
      "241 93\n",
      "241 94\n",
      "242 94\n",
      "243 94\n",
      "243 95\n",
      "244 95\n",
      "245 95\n",
      "246 95\n",
      "246 96\n",
      "246 97\n",
      "247 97\n",
      "248 97\n",
      "248 98\n",
      "249 98\n",
      "250 98\n",
      "251 98\n",
      "252 98\n",
      "253 98\n",
      "254 98\n",
      "255 98\n",
      "256 98\n",
      "257 98\n",
      "258 98\n",
      "259 98\n",
      "259 99\n",
      "260 99\n",
      "260 100\n",
      "261 100\n",
      "262 100\n",
      "263 100\n",
      "264 100\n",
      "265 100\n",
      "265 101\n",
      "266 101\n",
      "267 101\n",
      "267 102\n",
      "268 102\n",
      "269 102\n",
      "270 102\n",
      "270 103\n",
      "271 103\n",
      "272 103\n",
      "272 104\n",
      "273 104\n",
      "274 104\n",
      "275 104\n",
      "276 104\n",
      "277 104\n",
      "278 104\n",
      "278 105\n",
      "279 105\n",
      "280 105\n",
      "280 106\n",
      "280 107\n",
      "281 107\n",
      "281 108\n",
      "282 108\n",
      "283 108\n",
      "284 108\n",
      "285 108\n",
      "286 108\n",
      "287 108\n",
      "288 108\n",
      "288 109\n",
      "288 110\n",
      "288 111\n",
      "289 111\n",
      "290 111\n",
      "291 111\n",
      "292 111\n",
      "293 111\n",
      "294 111\n",
      "295 111\n",
      "295 112\n",
      "296 112\n",
      "297 112\n",
      "298 112\n",
      "299 112\n",
      "300 112\n",
      "301 112\n",
      "302 112\n",
      "302 113\n",
      "302 114\n",
      "303 114\n",
      "303 115\n",
      "304 115\n",
      "305 115\n",
      "306 115\n",
      "307 115\n",
      "308 115\n",
      "308 116\n",
      "309 116\n",
      "310 116\n",
      "311 116\n",
      "312 116\n",
      "312 117\n",
      "313 117\n",
      "314 117\n",
      "315 117\n",
      "316 117\n",
      "317 117\n",
      "318 117\n",
      "318 118\n",
      "319 118\n",
      "320 118\n",
      "321 118\n",
      "322 118\n",
      "323 118\n",
      "324 118\n",
      "325 118\n",
      "326 118\n",
      "327 118\n",
      "327 119\n",
      "328 119\n",
      "328 120\n",
      "329 120\n",
      "330 120\n",
      "331 120\n",
      "331 121\n",
      "332 121\n",
      "333 121\n",
      "334 121\n",
      "334 122\n",
      "335 122\n",
      "335 123\n",
      "336 123\n",
      "337 123\n",
      "337 124\n",
      "338 124\n",
      "339 124\n",
      "340 124\n",
      "341 124\n",
      "342 124\n",
      "343 124\n",
      "343 125\n",
      "344 125\n",
      "345 125\n",
      "346 125\n",
      "347 125\n",
      "348 125\n",
      "349 125\n",
      "350 125\n",
      "351 125\n",
      "351 126\n",
      "351 127\n",
      "351 128\n",
      "352 128\n",
      "353 128\n",
      "353 129\n",
      "354 129\n",
      "355 129\n",
      "356 129\n",
      "356 130\n",
      "356 131\n",
      "357 131\n",
      "358 131\n",
      "358 132\n",
      "359 132\n",
      "359 133\n",
      "360 133\n",
      "360 134\n",
      "361 134\n",
      "362 134\n",
      "363 134\n",
      "364 134\n",
      "364 135\n",
      "365 135\n",
      "366 135\n",
      "366 136\n",
      "367 136\n",
      "368 136\n",
      "368 137\n",
      "369 137\n",
      "369 138\n",
      "369 139\n",
      "370 139\n",
      "371 139\n",
      "372 139\n",
      "373 139\n",
      "373 140\n",
      "374 140\n",
      "375 140\n",
      "376 140\n",
      "377 140\n",
      "378 140\n",
      "379 140\n",
      "380 140\n",
      "380 141\n",
      "381 141\n",
      "381 142\n",
      "382 142\n",
      "383 142\n",
      "384 142\n",
      "385 142\n",
      "385 143\n",
      "386 143\n",
      "387 143\n",
      "387 144\n",
      "387 145\n",
      "388 145\n",
      "389 145\n",
      "390 145\n",
      "391 145\n",
      "392 145\n",
      "393 145\n",
      "394 145\n",
      "395 145\n",
      "395 146\n",
      "396 146\n",
      "397 146\n",
      "398 146\n",
      "399 146\n",
      "400 146\n",
      "401 146\n",
      "402 146\n",
      "402 147\n",
      "403 147\n",
      "404 147\n",
      "405 147\n",
      "406 147\n",
      "407 147\n",
      "408 147\n",
      "409 147\n",
      "410 147\n",
      "411 147\n",
      "412 147\n",
      "413 147\n",
      "413 148\n",
      "413 149\n",
      "413 150\n",
      "414 150\n",
      "414 151\n",
      "415 151\n",
      "416 151\n",
      "416 152\n",
      "416 153\n",
      "417 153\n",
      "418 153\n",
      "419 153\n",
      "420 153\n",
      "421 153\n",
      "421 154\n",
      "422 154\n",
      "422 155\n",
      "423 155\n",
      "424 155\n",
      "424 156\n",
      "424 157\n",
      "424 158\n",
      "425 158\n",
      "426 158\n",
      "427 158\n",
      "428 158\n",
      "429 158\n",
      "429 159\n",
      "430 159\n",
      "431 159\n",
      "432 159\n",
      "433 159\n",
      "434 159\n",
      "435 159\n",
      "435 160\n",
      "435 161\n",
      "436 161\n",
      "437 161\n",
      "438 161\n",
      "439 161\n",
      "439 162\n",
      "439 163\n",
      "439 164\n",
      "440 164\n",
      "440 165\n",
      "441 165\n",
      "441 166\n",
      "442 166\n",
      "443 166\n",
      "443 167\n",
      "443 168\n",
      "444 168\n",
      "445 168\n",
      "446 168\n",
      "447 168\n",
      "447 169\n",
      "448 169\n",
      "449 169\n",
      "449 170\n",
      "450 170\n",
      "451 170\n",
      "451 171\n",
      "452 171\n",
      "453 171\n",
      "454 171\n",
      "454 172\n",
      "455 172\n",
      "456 172\n",
      "457 172\n",
      "458 172\n",
      "459 172\n",
      "460 172\n",
      "461 172\n",
      "461 173\n",
      "462 173\n",
      "463 173\n",
      "463 174\n",
      "464 174\n",
      "464 175\n",
      "465 175\n",
      "465 176\n",
      "465 177\n",
      "466 177\n",
      "467 177\n",
      "467 178\n",
      "468 178\n",
      "469 178\n",
      "469 179\n",
      "470 179\n",
      "471 179\n",
      "472 179\n"
     ]
    }
   ],
   "source": [
    "train_xml, test_xml = 0, 0\n",
    "\n",
    "for xml_file in xml_list:\n",
    "    target_path = targetdir1 + \"\\\\\" + xml_file\n",
    "    targetXML = open(target_path, 'rt', encoding='UTF8')\n",
    "\n",
    "    tree = ET.parse(targetXML)\n",
    "\n",
    "    root = tree.getroot()\n",
    "\n",
    "    ##수정할 부분\n",
    "    name_tag = root.find(\"filename\")\n",
    "    name = name_tag.text\n",
    "    folder_tag = root.find(\"folder\")\n",
    "    path_tag = root.find(\"path\")\n",
    "    \n",
    "    if name in train_list:\n",
    "        original = path_tag.text     #원본 String\n",
    "        modified = original.replace(r\"C:\\ProgramData\\Anaconda3\\envs\\tf1\\Lib\\site-packages\\tensorflow\\models\\image\\class\",\n",
    "                                    \"/content/drive/MyDrive/ARmodel/models/image/train/class\")\n",
    "        modified = modified.replace(\"\\\\\", \"/\")\n",
    "\n",
    "        path_tag.text = modified  #수정\n",
    "\n",
    "        train_xml += 1\n",
    "    elif name in test_list:\n",
    "        original = path_tag.text     #원본 String\n",
    "        modified = original.replace(r\"C:\\ProgramData\\Anaconda3\\envs\\tf1\\Lib\\site-packages\\tensorflow\\models\\image\\class\",\n",
    "                                    \"/content/drive/MyDrive/ARmodel/models/image/val/class\")\n",
    "        modified = modified.replace(\"\\\\\", \"/\")\n",
    "\n",
    "        path_tag.text = modified  #수정\n",
    "\n",
    "        test_xml += 1\n",
    "    else: continue\n",
    "      \n",
    "    tree.write(target_path)\n",
    "    print(train_xml, test_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc9be8",
   "metadata": {},
   "source": [
    "### 5.3. Delete non-RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "074a6905",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image     \n",
    "\n",
    "train_del, test_del = [], []\n",
    "\n",
    "for file in train_list:      \n",
    "    extension = file.split('.')[-1]\n",
    "    fileLoc = traindir+\"\\\\\"+file\n",
    "    with Image.open(fileLoc) as img:\n",
    "        mode = img.mode\n",
    "    if mode != 'RGB':\n",
    "        train_del.append(file)\n",
    "        os.remove(fileLoc)\n",
    "        \n",
    "for file in test_list:      \n",
    "    extension = file.split('.')[-1]\n",
    "    fileLoc = testdir+\"\\\\\"+file\n",
    "    with Image.open(fileLoc) as img:\n",
    "        mode = img.mode\n",
    "    if mode != 'RGB':\n",
    "        test_del.append(file)\n",
    "        os.remove(fileLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01b6dd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 117\n"
     ]
    }
   ],
   "source": [
    "print(len(train_del), len(test_del))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a69ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_del = train_del + test_del\n",
    "xml_del_m = []\n",
    "\n",
    "for i in xml_del:\n",
    "    temp = i.replace('.jpg','.xml')\n",
    "    temp = temp.replace('.png','.xml')\n",
    "    temp = temp.replace('.jpeg','.xml')\n",
    "    xml_del_m.append(temp)\n",
    "    \n",
    "for file in os.listdir(targetdir1):\n",
    "    fileLoc = targetdir1+\"\\\\\"+file\n",
    "    if file in xml_del_m:\n",
    "        os.remove(fileLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06e00c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindir = r\"C:\\ProgramData\\Anaconda3\\envs\\tf1\\Lib\\site-packages\\tensorflow\\models\\image\\train\\class\"\n",
    "testdir = r\"C:\\ProgramData\\Anaconda3\\envs\\tf1\\Lib\\site-packages\\tensorflow\\models\\image\\val\\class\"\n",
    "\n",
    "all_list = os.listdir(traindir) + os.listdir(testdir)\n",
    "\n",
    "targetdir1 = r\"C:\\ProgramData\\Anaconda3\\envs\\tf1\\Lib\\site-packages\\tensorflow\\models\\annotations\\xmls\" # XML dir\n",
    "\n",
    "file_list = os.listdir(targetdir1)\n",
    "xml_list = []\n",
    "for file in file_list:\n",
    "    if '.xml' in file:\n",
    "        xml_list.append(file)\n",
    "\n",
    "for xml_file in xml_list:\n",
    "    target_path = targetdir1 + \"\\\\\" + xml_file\n",
    "    with open(target_path, 'rt', encoding='UTF8') as targetXML:\n",
    "        tree = ET.parse(targetXML)\n",
    "    root = tree.getroot()\n",
    "    name_tag = root.find(\"filename\")\n",
    "    name = name_tag.text\n",
    "    if name not in all_list:\n",
    "        os.remove(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec4322",
   "metadata": {},
   "source": [
    "### 5.4. xml to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e489a862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571 224\n"
     ]
    }
   ],
   "source": [
    "def xml_to_csv(path):\n",
    "    xml_train_list = []\n",
    "    xml_test_list = []\n",
    "\n",
    "    for xml_file in glob.glob(path + '\\\\*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        path_tag = root.find(\"path\")\n",
    "        for member in root.findall('object'):\n",
    "            value = (root.find('filename').text,\n",
    "                     int(root.find('size')[0].text),\n",
    "                     int(root.find('size')[1].text),\n",
    "                     member[0].text,\n",
    "                     int(member[4][0].text),\n",
    "                     int(member[4][1].text),\n",
    "                     int(member[4][2].text),\n",
    "                     int(member[4][3].text)\n",
    "                     )\n",
    "            if 'train' in path_tag.text:\n",
    "                xml_train_list.append(value)\n",
    "            else:\n",
    "                xml_test_list.append(value)\n",
    "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_train_df, xml_test_df = pd.DataFrame(xml_train_list, columns=column_name), pd.DataFrame(xml_test_list, columns=column_name)\n",
    "    return xml_train_df, xml_test_df\n",
    "\n",
    "xml_train_df, xml_test_df = xml_to_csv(targetdir1)\n",
    "xml_train_df.to_csv(r'C:\\ProgramData\\Anaconda3\\envs\\tf1\\Lib\\site-packages\\tensorflow\\models\\data\\train_labels.csv', index=None)\n",
    "xml_test_df.to_csv(r'C:\\ProgramData\\Anaconda3\\envs\\tf1\\Lib\\site-packages\\tensorflow\\models\\data\\test_labels.csv', index=None)\n",
    "\n",
    "print(xml_train_df.shape[0], xml_test_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc49f2d",
   "metadata": {},
   "source": [
    "### 5.5. Create tfrecord (@conda cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dbba47",
   "metadata": {},
   "source": [
    "(tf1) C:\\ProgramData\\Anaconda3\\envs\\tf1\\Lib\\site-packages\\tensorflow\\models>\n",
    "\n",
    "```\n",
    "python research\\object_detection\\dataset_tools\\generate_tfrecord.py --csv_input=data\\train_labels.csv --output_path=train.record --image_dir=image\\train\\class\n",
    "\n",
    "python research\\object_detection\\dataset_tools\\generate_tfrecord.py --csv_input=data\\test_labels.csv --output_path=test.record --image_dir=image\\val\\class\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50812b",
   "metadata": {},
   "source": [
    "## 6. Train (@conda cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b82606",
   "metadata": {},
   "source": [
    "```\n",
    "activate tf1\n",
    "\n",
    "cd C:\\ProgramData\\Anaconda3\\envs\\tf1\\Lib\\site-packages\\tensorflow\\models\n",
    "\n",
    "python research\\object_detection\\model_main.py --pipeline_config_path=ssd_mobilenet_v2_coco.config --model_dir=train --num_train_steps=20000 --sample_1_of_n_eval_examples=1 --alsologtostderr\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f99ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"C:\\Users\\Hyoungjoon LIM\\Desktop\\scene_text_classification\\paired_image\"\n",
    "\n",
    "image_list = os.listdir(image_dir)\n",
    "\n",
    "xml_dir = r\"C:\\Users\\Hyoungjoon LIM\\Desktop\\scene_text_classification\\all_xml\" # XML dir\n",
    "\n",
    "file_list = os.listdir(xml_dir)\n",
    "xml_list = []\n",
    "\n",
    "for file in file_list:\n",
    "    xml_path = xml_dir + \"\\\\\" + file\n",
    "    with open(xml_path, 'rt', encoding='UTF8') as targetXML:\n",
    "        tree = ET.parse(targetXML)\n",
    "    root = tree.getroot()\n",
    "    name_tag = root.find(\"filename\")\n",
    "    name = name_tag.text\n",
    "    xml_list.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95934557",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in image_list:\n",
    "    target_path = image_dir + \"\\\\\" + image\n",
    "    if image not in xml_list:\n",
    "        os.remove(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d98d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
